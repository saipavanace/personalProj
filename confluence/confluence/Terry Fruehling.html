<p>07 Sept 2022</p><ul><li> Software tool IP development flow criteria evaluation report:</li><ul><li>New graphics describing Maestro/NCore Development Process</li><ul><li>One graphic focused on the Arteris Verification groups &amp; Maestro input into the test bench</li><li>The second graphic focuses on the test bench, and the files linked and compiled by the Questa Sim Compiler</li><li>These diagrams were reviewed by the team and several corrections were made.</li></ul><li>The purpose of the new graphics was to perform the following:</li><ul><li>highlight the source of common cause errors into the Questa Sim compiler and understand their detectability</li><li>Theorize possible fault escapes that could be sourced by the Questa Sim Compiler, and how they might be detected by support tools in the Maestro development flow.</li><ul><li>Note:  Multiple emails sent to Mentor relative to this task were sent requesting information regarding Questa tool Safety manuals, or fault modes of simulator, unfortunately, needed information was not available.</li><li>Abstract concepts of Questa Sim compiler failure were discussed in an abstract nature, however producing actual evidence of failure capture by the tool flow remains elusive.</li><li>There are several identified sources of common cause failure that can be sourced either from Maestro tool, the IP verification plan, or UVM Test Bench. However,  if the compiled object file is corrupted by the Questa Sim compiler in a way that escapes the Questa tool diagnostics (unreported tool error),  the UVM test bench probably will not have the observation points to detect the failure.</li></ul></ul><li>Actions</li><ul><li>Several Arteris Tool Flow ISO Compliance meetings with S. Lorenzini, A. Hebbar, &amp; T. Fruehling:</li><ul><li><em>The option of claiming the Maestro IP development tool flow as TCL2 and then refering users requiring the TCL1 rating, to use the tool flow as described and substantiated by Siemens (or Synopsys) was rejected.</em></li><li><em>Arteris is committed to developing its own TD1/TCL1 tool flow and generating its own evidence to substantiate Arteris tool flow proofs. </em></li><li><em>As a function of proof design- the client will be required to send back their Ncore implementation for Arteris full regression testing.</em></li><ul><li><em>However, the client will only send back the Ncore implementation that they created,  not their entire SOC implementation.</em></li><li><em>This will not include the client verification environment. This could encounter encumberments with client or third-party IP of which we have no knowledge</em></li><li><em>Hence the client independently evaluates their IP in absence of Arteris regression tests, and Arteris will evaluate client system in absence of the client’s regression tests. </em></li></ul></ul><li>A. Hebbar suggested that it might be possible to demonstrate a compiled error in the stimulus model as detectable.  Akarsh volunteered to develop this concept for review.</li><li>T. Fruehling will investigate overlap between the tool flow formal test, and library component functional equivalence checks – to see if this will support ISO 26262 Tool Error Detection level 1 (TD1) compliance.</li><li>T. Fruehling will review other tools in the flow, code coverage, linting etc.</li><li>T. Fruehling investigating regression testing effectiveness regarding client requirment to achieve higher tool confidence rating and safety ASIL D rating.</li><ul><li>Several Meetings, email and texts.  with J. Coddington – the following is a synopsis of these discussions:</li><ul><li>Regression testing Improves Quality of design – Specific tool flow TD1, TCL1, safety aspects remain undetermined with current level of analysis.  the regression testing does support the design capability analysis,  but it may not be sufficient to conclude the lifting the tool flow error detection capability to the necessary TD1, TCL1 rating.  (I.e. you may find errors in the design, but not in unreported tool errors).</li><li>Possibly considered as final end-to-end test,  but does not lift TCL2 to a TCL1 rating</li><ul><li>Nothing inflow covers unreported errors in tools</li><ul><li>The only way this is done is by analyzing final performance anomalies, when they arise – this is end-to-end testing, however this not one tool capturing unreported faults in another tool.</li><li>This is random, ad-hoc fault detection.  Since the faults are unknown, observation points cannot be deterministically assessed.  It depends on end-to-end performance testing and relies on the surfacing of design or tool anomalies prior to product release.   The anomalies be mostly attributable to design inaccuracies.  These are not in the same category as tool anomalies, which is what we are hunting for in commercial tools.</li></ul><li>We place customer requirements into a system of Arteris’s design, not customer system –</li><ul><li>Running our Test Bench on Customer Test Bench is not possible</li><li>Arteris creates a customer test bench that the customer can run, but it is not an Arteris test bench that is integrated into their testbench.</li><li>The client can take parts of the test bench and integrate it into their test bench – which Arteris AEs support</li><li>However, the supplied collateral to the client, doesn’t have the same level of error detection.</li><li>Actual customer system design could still cause malfunction – not captured in Arteris extensive regression testing, further this relates to a design inconsistency, not a tool generated error.</li><li>Arteris has protocol monitors that are not shared with customer, again this is a design function, not a tool generated malfunction..  </li><li>Arteris supplies client with collaterals which can be integrated into client test benches, however Arteris does not do the integration, nor does Arteris run the tests,  this process cannot be used to discern between design vs. tool related faults. </li></ul></ul></ul></ul></ul></ul></ul><p><br/></p><p>19 08 2022</p><ul><li> Software tool IP development flow criteria evaluation report<ul><li>Individual and group meetings held with each to clarify certification task, relative to Exida certification assessment documentation</li></ul><ul><li>Direction to use one tool of the Arteris tool flow, and one module as an example that can be extrapolated to demonstrate compliance coverage.</li></ul></li><ul><li>3<sup>nd</sup> draft, rev0.10. distributed to S. Lorenzini , A. Hebbar &amp; J. Coddington, S. Prakash, S. Zahid, B. Chen</li><li>Meeting with S. Lorenzini, A. Hebbar, to streamline certification task</li><li>Reviewing Arteris and Questa tool files to understand tool failure modes and embedded tool diagnostics</li><li>Weekly reviews with S.L and A.H.</li></ul></ul><p>29 Jul 2022</p><ul><li> Software tool IP development flow criteria evaluation report</li><ul><li>2<sup>nd</sup> draft distributed to S. Lorenzini , A. Hebbar &amp; John Coddington.</li><li>Progress – completion of executive summary and Maestro tool error detection (TD) and confidence level (TCL).</li><li>Weekly reviews with S.L and A.H.</li></ul></ul><p>20 Jul 2022</p><ul><li> Software tool IP development flow criteria evaluation report</li><ul><li>First restricted draft distributed to S. Lorenzini &amp; A. Hebbar.</li><li>Progress adding &amp; improving report content and readability</li><li>Weekly reviews with S.L and A.H.</li></ul></ul><p>13 Jul 2022</p><ul><li> Software tool IP development flow criteria evaluation report</li><ul><li>Rough draft reviewed with S. Lorenzini</li><li>Two of 7 sections nearly complete- presently in revision</li><li> Framework of entire report established and approved by S. Lorenzini</li><li>Weekly reviews with S. Lorenzini<br/><br/></li></ul></ul><p>30  June 2022</p><ol><li>Developing Software tool IP development flow criteria evaluation report</li><ul><li>Progressing on Intro and Maestro sections   <br/><br/></li></ul></ol><p>22 June 2022</p><ol><li>Tool Flow CL1 Software tool criteria evaluation report  </li><ul><li>Plan to create 6 sections report created and approved target completion 10 to 12 weeks</li><li>First section focusing on common cause errors due to legal structural, or functional parameters has been started.</li></ul></ol><p><br/></p><p>09 June 2022</p><ol><li>Maestro Common Cause Error Resolution strategy</li><ol><li>Stefano offered up a host of Maestro common cause faults (CCF) that may legitimately be captured by IP development flow</li><li>However, CCFs will still persist. – Goal will be to show that these CCFs, would be a relatively low risk of occurrence.<br/><br/></li></ol><li>Maestro Actions<ol><li>Propose suggestions to improve confidence in Stefano CCF spreadsheet</li><li>Design team meeting(s) with design group will be held, in order to achieve the following:<ol><li>To ensure arguments presented in Stefano CCF are correct and sufficient</li><li>To enhance/strengthen arguments presented in Stefano CCF</li></ol></li></ol></li></ol><p><br/></p><p>01 June 2022</p><ol><li>Maestro Common Cause Error Resolution<ol><ul><li>Though there are intermediate consistency checks within the Maestro tool processes,  at present, they are insufficient to totally resolve the common cause error problem as indicated by Exida.</li><li>A corollary issue also exists,  such that, if an error exists in the client file,  and the test bench does not have the fidelity necessary to discover the associated error in the RTL File<ul><li>These may result in safe faults such as:<ul><li>Configuration errors, which could result in a reduced architectural optimization – i.e. resulting in increasing the area and power consumption of the NOC.</li><li>Non-safe or latent faults:<ol><li>These examples have not been explored.</li></ol></li></ul></li></ul></li></ul></ol></li><ol><li>Meeting held May 31, 2022. – Attendees – S. Lorenzini, S. Prakash, E. Howard, J. Coddington, B. Chen, T. Fruehling.</li><li>The group determined that a gap does exist from the user entry point into which can be reflected in both the exported RTL and Testbench files which are exported from the Maestro JSON files.</li></ol></ol><ol><li>Actions:</li><ol><li>Stefano L.  has offered to produce examples of Maestro consistancy checks which will reduce some likely common cause errors in Maestro</li><li>A general statement will need to be added to the Maestro safety / user manual which indicates:<ol><li>The capabilities &amp; limitations of the Maestro tool to detect common cause failures</li><li>Examples of the types of errors Maestro tool will detect.</li><li>The customer is responsibility to ensure proper performance of their IP and associated safety mechanisms using downstream flow tools from Maestro and their own analysis or simulations.</li></ol></li></ol></ol><p>        Notes:</p><ol><ol><li>At this time performance testing of the final IP integration is the only solution available and needs to be resolved by the customer - Though the Arteris team will continue to investigate other options to improve the Maestro tool.</li><li>it was determined that returning client IP back to Arteris for regression testing <em><u>would not</u></em> be an effective method to detect internal CC errors in the client IP</li><li>The use of a tool called Aria would not be sufficient redundancy to capture common cause faults, because of its present integration into the Maestro tool.</li></ol></ol><p><br/></p><p><br/></p><p>25 May 2022</p><p><br/></p><ol><li>Resolve Exida Safety deviations/non conformances with tool flow – focus on Maestro.</li><ol><li>Several internal discussions regarding tool diagnostic effectiveness and gaps which need to be resolved to receive Exida ISO 26262 compliance certification.</li><li>General meeting with stakeholders this Friday to discuss open issues &amp; propose solutions.</li></ol><li>Working through Ncore safety verification Plan, Safety manual, Ncore (Maestro) user guide, etc.<ol><li>Stake holder meeting to day to discuss Ncore Verification Plan</li></ol></li></ol><p><br/></p><p><br/></p><p style="text-align: left;"><br/></p>