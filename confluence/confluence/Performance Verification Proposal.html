<style type='text/css'>/*<![CDATA[*/
div.rbtoc1759726143301 {padding: 0px;}
div.rbtoc1759726143301 ul {list-style: circle;margin-left: 0px;}
div.rbtoc1759726143301 li {margin-left: 0px;padding-left: 0px;}

/*]]>*/</style><div class='toc-macro rbtoc1759726143301'>
<ul class='toc-indentation'>
<li><a href='#PerformanceVerificationProposal-Introduction'>Introduction</a></li>
<li><a href='#PerformanceVerificationProposal-Algorithm'>Algorithm</a>
<ul class='toc-indentation'>
<li><a href='#PerformanceVerificationProposal-ProblemSetupandMathematicalFoundation'>Problem Setup and Mathematical Foundation</a>
<ul class='toc-indentation'>
<li><a href='#PerformanceVerificationProposal-BasicProblemFormulation'>Basic Problem Formulation</a></li>
<li><a href='#PerformanceVerificationProposal-TheLinearAssumptionProblem'>The Linear Assumption Problem</a></li>
<li><a href='#PerformanceVerificationProposal-Non-LinearSolution'>Non-Linear Solution</a></li>
</ul>
</li>
<li><a href='#PerformanceVerificationProposal-MathematicalPreliminaries'>Mathematical Preliminaries</a>
<ul class='toc-indentation'>
<li><a href='#PerformanceVerificationProposal-NotationandConventions'>Notation and Conventions</a></li>
<li><a href='#PerformanceVerificationProposal-KeyMathematicalObjects'>Key Mathematical Objects</a></li>
</ul>
</li>
<li><a href='#PerformanceVerificationProposal-TheKernelTrick'>The Kernel Trick</a>
<ul class='toc-indentation'>
<li><a href='#PerformanceVerificationProposal-CoreInsight'>Core Insight</a></li>
<li><a href='#PerformanceVerificationProposal-MagicIdentity'>Magic Identity</a></li>
<li><a href='#PerformanceVerificationProposal-FromLineartoKernelRLS'>From Linear to Kernel RLS</a></li>
<li><a href='#PerformanceVerificationProposal-TheRepresenterTheorem'>The Representer Theorem</a></li>
<li><a href='#PerformanceVerificationProposal-PredictioninDualForm'>Prediction in Dual Form</a></li>
<li><a href='#PerformanceVerificationProposal-KernelRLSUpdateEquations'>Kernel RLS Update Equations</a></li>
<li><a href='#PerformanceVerificationProposal-TheGrowingMemoryProblem'>The Growing Memory Problem</a></li>
</ul>
</li>
<li><a href='#PerformanceVerificationProposal-RandomFourierFeatures'>Random Fourier Features</a>
<ul class='toc-indentation'>
<li><a href='#PerformanceVerificationProposal-TheFundamentalProblem'>The Fundamental Problem</a></li>
<li><a href='#PerformanceVerificationProposal-Bochner&#39;sTheoremFoundation'>Bochner&#39;s Theorem Foundation</a></li>
<li><a href='#PerformanceVerificationProposal-TheRBFKernelCase'>The RBF Kernel Case</a></li>
<li><a href='#PerformanceVerificationProposal-RandomFourierFeaturesConstruction'>Random Fourier Features Construction</a></li>
<li><a href='#PerformanceVerificationProposal-TheApproximationQuality'>The Approximation Quality</a></li>
<li><a href='#PerformanceVerificationProposal-WhyThisSolvesOurProblem'>Why This Solves Our Problem</a></li>
<li><a href='#PerformanceVerificationProposal-AlternativeRFFConstructions'>Alternative RFF Constructions</a></li>
<li><a href='#PerformanceVerificationProposal-HyperparameterSelection'>Hyperparameter Selection</a></li>
</ul>
</li>
<li><a href='#PerformanceVerificationProposal-TheCombinedRLS+RFFAlgorithm'>The Combined RLS+RFF Algorithm</a>
<ul class='toc-indentation'>
<li><a href='#PerformanceVerificationProposal-ProblemReformulation'>Problem Reformulation</a></li>
<li><a href='#PerformanceVerificationProposal-AlgorithmInitialization'>Algorithm Initialization</a></li>
<li><a href='#PerformanceVerificationProposal-OnlineUpdateAlgorithm'>Online Update Algorithm</a></li>
<li><a href='#PerformanceVerificationProposal-PredictionforNewInput'>Prediction for New Input</a></li>
<li><a href='#PerformanceVerificationProposal-CompleteAlgorithmSummary'>Complete Algorithm Summary</a></li>
<li><a href='#PerformanceVerificationProposal-ComplexityAnalysis'>Complexity Analysis</a></li>
</ul>
</li>
</ul>
</li>
<li><a href='#PerformanceVerificationProposal-FeatureSet'>Feature Set</a>
<ul class='toc-indentation'>
<li><a href='#PerformanceVerificationProposal-Requirement'>Requirement</a></li>
<li><a href='#PerformanceVerificationProposal-Configuration'>Configuration</a></li>
<li><a href='#PerformanceVerificationProposal-GlobalParameters'>Global Parameters</a></li>
<li><a href='#PerformanceVerificationProposal-TransactionAttributes'>Transaction Attributes</a></li>
<li><a href='#PerformanceVerificationProposal-CacheStates'>Cache States</a></li>
<li><a href='#PerformanceVerificationProposal-SnoopResponses'>Snoop Responses</a></li>
<li><a href='#PerformanceVerificationProposal-ChildTransactions'>Child Transactions</a></li>
<li><a href='#PerformanceVerificationProposal-StateSnapshot'>State Snapshot</a></li>
<li><a href='#PerformanceVerificationProposal-PerformanceMetrics'>Performance Metrics</a></li>
<li><a href='#PerformanceVerificationProposal-ExampleFeatureSet'>Example Feature Set</a></li>
</ul>
</li>
<li><a href='#PerformanceVerificationProposal-BlockDiagram'>Block Diagram</a></li>
<li><a href='#PerformanceVerificationProposal-Flow'>Flow</a></li>
<li><a href='#PerformanceVerificationProposal-Training'>Training</a>
<ul class='toc-indentation'>
<li><a href='#PerformanceVerificationProposal-BulkTraining'>Bulk Training</a></li>
<li><a href='#PerformanceVerificationProposal-ModelUpdate'>Model Update</a></li>
</ul>
</li>
<li><a href='#PerformanceVerificationProposal-ExternalScripts'>External Scripts</a></li>
<li><a href='#PerformanceVerificationProposal-Challenges'>Challenges</a></li>
<li><a href='#PerformanceVerificationProposal-NextSteps'>Next Steps</a></li>
</ul>
</div><h1 id="PerformanceVerificationProposal-Introduction">Introduction</h1><p>This proposal outlines a self-learning performance verification methodology for our configurable Network Interconnect IP. The approach uses Online Kernelized Recursive Least Squares (RLS) with Random Fourier Features (RFF) algorithm to predict expected values of key performance metrics at every internal and external interfaces, provide uncertainty bounds, and automatically fail UVM tests when observed metrics exceed predicted expectations beyond a tunable threshold. It also pinpoints bottlenecks along the transaction path via residual-based localization reinforced by pressure indicators (queue depth, utilization, stalls, cache miss rate), enabling rapid root-cause debugging and actionable architectural insights.</p><h1 id="PerformanceVerificationProposal-Algorithm">Algorithm</h1><h2 id="PerformanceVerificationProposal-ProblemSetupandMathematicalFoundation">Problem Setup and Mathematical Foundation</h2><h3 id="PerformanceVerificationProposal-BasicProblemFormulation">Basic Problem Formulation</h3><p>We have a nonlinear regression problem where we want to learn the relationship between input features and target values:</p><p><strong>Input Feature Vector:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">x = [x₁, x₂, x₃, x₄, ..., xₙ]ᵀ ∈ ℝⁿ</pre>
</div></div><p><strong>Target Parameter Vector:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">θ = [θ₁, θ₂, θ₃, ..., θₘ] ∈ ℝᵐ</pre>
</div></div><p><strong>Objective:</strong> Learn the mapping function f such that:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">y = f(x) + ε</pre>
</div></div><p>where:</p><ul><li><p>y ∈ ℝ is the scalar output we want to predict</p></li><li><p>ε is additive noise</p></li><li><p>f: ℝⁿ → ℝ is an unknown nonlinear function</p></li></ul><h3 id="PerformanceVerificationProposal-TheLinearAssumptionProblem">The Linear Assumption Problem</h3><p>Standard Recursive Least Squares (RLS) assumes a linear relationship:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">y = xᵀθ + ε</pre>
</div></div><p>This works well when the true relationship is approximately linear, but fails catastrophically for nonlinear systems.</p><p><strong>Key Limitation:</strong> In standard RLS, we have n input features and m parameters where typically m = n (one parameter per feature). This restricts us to linear combinations only.</p><h3 id="PerformanceVerificationProposal-Non-LinearSolution">Non-Linear Solution</h3><p>To handle nonlinear relationships, we need to transform our approach. The core insight is to map our input features to a higher-dimensional space where the relationship becomes linear.</p><p><strong>The Transformation Strategy:</strong></p><ol start="1"><li><p>Map inputs to a feature space: x → φ(x)</p></li><li><p>Apply linear methods in the feature space</p></li><li><p>The result captures nonlinear relationships in the original space</p></li></ol><p>This is where kernel methods and Random Fourier Features come into play.</p><h2 id="PerformanceVerificationProposal-MathematicalPreliminaries">Mathematical Preliminaries</h2><h3 id="PerformanceVerificationProposal-NotationandConventions">Notation and Conventions</h3><p>Throughout this document, we use the following notation:</p><ul><li><p><strong>Vectors:</strong> Bold lowercase (x, θ, φ)</p></li><li><p><strong>Matrices:</strong> Bold uppercase (P, K, W)</p></li><li><p><strong>Scalars:</strong> Regular text (y, λ, γ)</p></li><li><p><strong>Transpose:</strong> Superscript ᵀ</p></li><li><p><strong>Time index:</strong> Subscript or parentheses: x(t) or xₜ</p></li><li><p><strong>Dimensionality:</strong></p><ul><li><p>n = original input dimension</p></li><li><p>m = number of parameters (will be redefined for kernel methods)</p></li><li><p>D = number of Random Fourier Features</p></li></ul></li></ul><h3 id="PerformanceVerificationProposal-KeyMathematicalObjects">Key Mathematical Objects</h3><p><strong>Input Space:</strong> 𝒳 ⊆ ℝⁿ (where our original features live)</p><p><strong>Feature Space:</strong> ℋ (potentially infinite-dimensional Hilbert space)</p><p><strong>Feature Map:</strong> φ: 𝒳 → ℋ (transforms inputs to feature space)</p><p><strong>Kernel Function:</strong> K: 𝒳 × 𝒳 → ℝ where K(x, x') = ⟨φ(x), φ(x')⟩ℋ</p><h2 id="PerformanceVerificationProposal-TheKernelTrick">The Kernel Trick</h2><h3 id="PerformanceVerificationProposal-CoreInsight">Core Insight</h3><p>The kernel trick allows us to work in infinite-dimensional feature spaces without ever explicitly computing the feature vectors. Instead of computing φ(x), we only compute inner products ⟨φ(x), φ(x')⟩.</p><h3 id="PerformanceVerificationProposal-MagicIdentity">Magic Identity</h3><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">K(x, x&#39;) = ⟨φ(x), φ(x&#39;)⟩ℋ</pre>
</div></div><p>This means we can capture complex nonlinear relationships using only pairwise kernel evaluations.</p><h3 id="PerformanceVerificationProposal-FromLineartoKernelRLS">From Linear to Kernel RLS</h3><p><strong>Standard Linear RLS Model:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">y = xᵀθ + ε</pre>
</div></div><p><strong>Kernelized Model:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">y = φ(x)ᵀω + ε</pre>
</div></div><p>where:</p><ul><li><p>φ(x) ∈ ℝᴰ is the feature map (potentially infinite-dimensional)</p></li><li><p>ω ∈ ℝᴰ are the parameters in feature space</p></li></ul><h3 id="PerformanceVerificationProposal-TheRepresenterTheorem">The Representer Theorem</h3><p><strong>Key Theoretical Result:</strong> The optimal solution ω* can always be written as a linear combination of the feature vectors:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">ω* = Σᵢ₌₁ᵗ αᵢ φ(xᵢ)</pre>
</div></div><p>where {x₁, x₂, ..., xₜ} are all the data points seen so far.</p><p><strong>Implication:</strong> Instead of learning ω directly, we can learn the coefficients α = [α₁, α₂, ..., αₜ]ᵀ.</p><h3 id="PerformanceVerificationProposal-PredictioninDualForm">Prediction in Dual Form</h3><p><strong>Original Form:</strong> y = φ(x)ᵀω</p><p><strong>Dual Form:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">y = φ(x)ᵀ(Σᵢ₌₁ᵗ αᵢ φ(xᵢ))
  = Σᵢ₌₁ᵗ αᵢ φ(x)ᵀφ(xᵢ)
  = Σᵢ₌₁ᵗ αᵢ K(x, xᵢ)</pre>
</div></div><p><strong>This is the kernel trick in action!</strong> We never compute φ(x) explicitly, only kernel evaluations K(x, xᵢ).</p><h3 id="PerformanceVerificationProposal-KernelRLSUpdateEquations">Kernel RLS Update Equations</h3><p>At time step t, when we receive new data (x(t), y(t)):</p><p><strong>Define the kernel vector:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">k(t) = [K(x(t), x(1)), K(x(t), x(2)), ..., K(x(t), x(t-1))]ᵀ ∈ ℝᵗ⁻¹</pre>
</div></div><p><strong>Kernel RLS Updates:</strong></p><p><strong>Step 1: Kalman Gain</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">g(t) = P(t-1) k(t) / (λ + kᵀ(t) P(t-1) k(t))</pre>
</div></div><p><strong>Step 2: Update Coefficients</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">α(t) = [α(t-1); 0] + g̃(t) [y(t) - kᵀ(t) α(t-1)]</pre>
</div></div><p>where g̃(t) = [g(t); -1] (extended Kalman gain)</p><p><strong>Step 3: Update Covariance</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">P(t) = (1/λ) [P̃(t-1) - g̃(t) k̃ᵀ(t) P̃(t-1)]</pre>
</div></div><p>where P̃(t-1) and k̃(t) are appropriately extended matrices.</p><h3 id="PerformanceVerificationProposal-TheGrowingMemoryProblem">The Growing Memory Problem</h3><p><strong>Critical Issue:</strong> The kernel vector k(t) grows with each time step!</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">Time t=1: k(1) = [] (empty vector)
Time t=2: k(2) = [K(x(2), x(1))] ∈ ℝ¹
Time t=3: k(3) = [K(x(3), x(1)), K(x(3), x(2))] ∈ ℝ²
...
Time t=T: k(T) ∈ ℝᵀ⁻¹</pre>
</div></div><p><strong>Memory Complexity:</strong> O(T²) - becomes impractical for long sequences!</p><p><strong>Computational Complexity:</strong> O(T²) per update - slows down over time!</p><p>This is exactly why we need Random Fourier Features...</p><h2 id="PerformanceVerificationProposal-RandomFourierFeatures">Random Fourier Features</h2><h3 id="PerformanceVerificationProposal-TheFundamentalProblem">The Fundamental Problem</h3><p>Kernel methods are powerful but suffer from the <strong>curse of growing complexity</strong>:</p><ul><li><p>Memory: O(T²)</p></li><li><p>Computation: O(T²) per update</p></li><li><p>Storage: Must keep all historical data points</p></li></ul><p><strong>Question:</strong> Can we approximate infinite-dimensional kernel mappings with fixed-size features?</p><p><strong>Answer:</strong> Yes! Random Fourier Features provide this approximation.</p><h3 id="PerformanceVerificationProposal-Bochner&#39;sTheoremFoundation">Bochner's Theorem Foundation</h3><p><strong>Bochner's Theorem:</strong> A continuous, translation-invariant kernel K(x, x') = K(x - x') corresponds to the Fourier transform of a non-negative measure.</p><p>For shift-invariant kernels, we can write:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">K(x - x&#39;) = ∫ p(ω) e^(iωᵀ(x-x&#39;)) dω</pre>
</div></div><p>where p(ω) is a probability density function in frequency domain.</p><h3 id="PerformanceVerificationProposal-TheRBFKernelCase">The RBF Kernel Case</h3><p><strong>Radial Basis Function (RBF) Kernel:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">K(x, x&#39;) = exp(-γ ||x - x&#39;||²)</pre>
</div></div><p><strong>Corresponding Frequency Distribution:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">p(ω) = (2πσ²)^(-n/2) exp(-||ω||²/(2σ²))</pre>
</div></div><p>where σ² = 2γ (Gaussian distribution in frequency domain).</p><h3 id="PerformanceVerificationProposal-RandomFourierFeaturesConstruction">Random Fourier Features Construction</h3><p><strong>The Approximation Strategy:</strong></p><p>Instead of working with infinite-dimensional φ(x), we construct a finite D-dimensional approximation.</p><p><strong>Step 1: Sample Random Frequencies</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">ωⱼ ~ p(ω) = 𝒩(0, 2γI)  for j = 1, 2, ..., D</pre>
</div></div><p><strong>Step 2: Sample Random Phases</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">bⱼ ~ Uniform[0, 2π]  for j = 1, 2, ..., D</pre>
</div></div><p><strong>Step 3: Construct Feature Map</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">φ_RFF(x) = √(2/D) [cos(ω₁ᵀx + b₁), cos(ω₂ᵀx + b₂), ..., cos(ωDᵀx + bD)]ᵀ</pre>
</div></div><h3 id="PerformanceVerificationProposal-TheApproximationQuality">The Approximation Quality</h3><p><strong>Theoretical Guarantee:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">𝔼[φ_RFF(x)ᵀφ_RFF(x&#39;)] = K(x, x&#39;)</pre>
</div></div><p>The expectation is taken over the random draws of ω and b.</p><p><strong>Approximation Error Bound:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">|K(x, x&#39;) - φ_RFF(x)ᵀφ_RFF(x&#39;)| = O(D^(-1/2))</pre>
</div></div><p><strong>Key Insight:</strong> The approximation error decreases as O(D^(-1/2)), so doubling the features cuts error by ~30%.</p><h3 id="PerformanceVerificationProposal-WhyThisSolvesOurProblem">Why This Solves Our Problem</h3><p><strong>Fixed Dimensionality:</strong> φ_RFF(x) ∈ ℝᴰ always, regardless of how much data we've seen.</p><p><strong>Memory:</strong> O(D²) instead of O(T²)</p><p><strong>Computation:</strong> O(D²) per update instead of O(T²)</p><p><strong>Quality Control:</strong> We can choose D to balance approximation quality vs. computational cost.</p><h3 id="PerformanceVerificationProposal-AlternativeRFFConstructions">Alternative RFF Constructions</h3><p><strong>Variant 1: Sine and Cosine Features</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">φ_RFF(x) = (1/√D) [cos(ω₁ᵀx), sin(ω₁ᵀx), ..., cos(ωD/2ᵀx), sin(ωD/2ᵀx)]ᵀ</pre>
</div></div><p><strong>Variant 2: Complex Exponentials (then take real part)</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">φ_RFF(x) = √(2/D) ℜ[e^(iω₁ᵀx), e^(iω₂ᵀx), ..., e^(iωDᵀx)]ᵀ</pre>
</div></div><p><strong>Variant 3: Structured Random Features</strong> Use structured matrices (e.g., circulant, Toeplitz) for ω to reduce randomness and improve consistency.</p><h3 id="PerformanceVerificationProposal-HyperparameterSelection">Hyperparameter Selection</h3><p><strong>Number of Features D:</strong></p><ul><li><p>More features → better approximation, higher computational cost</p></li><li><p>Rule of thumb: D ∈ [50, 500] for most applications</p></li><li><p>Theoretical guidance: D ≥ O(n log(1/ε)) for ε approximation error</p></li></ul><p><strong>Kernel Bandwidth γ:</strong></p><ul><li><p>Controls the &quot;smoothness&quot; of the learned function</p></li><li><p>Small γ → smooth, global patterns</p></li><li><p>Large γ → detailed, local patterns</p></li><li><p>Often chosen via cross-validation</p></li></ul><h2 id="PerformanceVerificationProposal-TheCombinedRLS+RFFAlgorithm">The Combined RLS+RFF Algorithm</h2><h3 id="PerformanceVerificationProposal-ProblemReformulation">Problem Reformulation</h3><p>With Random Fourier Features, we transform our nonlinear problem into a linear problem in RFF space:</p><p><strong>Original Nonlinear Problem:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">y = f(x) + ε  (unknown nonlinear function)</pre>
</div></div><p><strong>RFF Linear Problem:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">y = φ_RFF(x)ᵀθ + ε</pre>
</div></div><p>where:</p><ul><li><p>φ_RFF(x) ∈ ℝᴰ (fixed-size feature vector)</p></li><li><p>θ ∈ ℝᴰ (parameters in RFF space)</p></li></ul><p><strong>Crucial Change:</strong> Our parameter vector θ now has dimension D (number of RFF features), not n (original input dimension).</p><h3 id="PerformanceVerificationProposal-AlgorithmInitialization">Algorithm Initialization</h3><p><strong>Random Feature Generation (Done Once):</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">For j = 1, 2, ..., D:
    ωⱼ ~ 𝒩(0, 2γI_n)     // Random frequencies
    bⱼ ~ Uniform[0, 2π]    // Random phases</pre>
</div></div><p><strong>RLS Initialization:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">θ(0) = 0_D                // Zero initial parameters
P(0) = δI_D               // Initial covariance (δ = large number, e.g., 1000)</pre>
</div></div><h3 id="PerformanceVerificationProposal-OnlineUpdateAlgorithm">Online Update Algorithm</h3><p>When new data (x(t), y(t)) arrives at time t:</p><p><strong>Step 1: Compute RFF Features</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">φ(t) = √(2/D) [cos(ω₁ᵀx(t) + b₁)]
                [cos(ω₂ᵀx(t) + b₂)]
                [        ⋮        ]
                [cos(ωDᵀx(t) + bD)]</pre>
</div></div><p><strong>Step 2: Kalman Gain Vector</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">K(t) = P(t-1) φ(t) / (λ + φᵀ(t) P(t-1) φ(t)) ∈ ℝᴰ</pre>
</div></div><p><strong>Step 3: Prediction and Error</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">ŷ(t) = φᵀ(t) θ(t-1)                    // Current prediction
e(t) = y(t) - ŷ(t)                     // Prediction error</pre>
</div></div><p><strong>Step 4: Parameter Update</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">θ(t) = θ(t-1) + K(t) e(t) ∈ ℝᴰ</pre>
</div></div><p><strong>Step 5: Covariance Update</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">P(t) = (1/λ) [P(t-1) - K(t) φᵀ(t) P(t-1)] ∈ ℝᴰˣᴰ</pre>
</div></div><h3 id="PerformanceVerificationProposal-PredictionforNewInput">Prediction for New Input</h3><p>For a new input x_new:</p><p><strong>Step 1: Compute RFF features</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">φ_new = φ_RFF(x_new)</pre>
</div></div><p><strong>Step 2: Make prediction</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">y_pred = φᵀ_new θ(current)</pre>
</div></div><p><strong>Step 3: Compute prediction uncertainty</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">σ²_pred = φᵀ_new P(current) φ_new</pre>
</div></div><h3 id="PerformanceVerificationProposal-CompleteAlgorithmSummary">Complete Algorithm Summary</h3><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">ALGORITHM: RLS with Random Fourier Features

INPUT: Stream of data {(x(t), y(t))}_{t=1}^∞
PARAMETERS: D (features), γ (bandwidth), λ (forgetting factor)

INITIALIZATION:
    Sample {ωⱼ, bⱼ}_{j=1}^D
    θ(0) ← 0_D
    P(0) ← δI_D

FOR each new data point (x(t), y(t)):
    φ(t) ← φ_RFF(x(t))                              // O(nD)
    K(t) ← P(t-1) φ(t) / (λ + φᵀ(t) P(t-1) φ(t))   // O(D²)
    e(t) ← y(t) - φᵀ(t) θ(t-1)                     // O(D)
    θ(t) ← θ(t-1) + K(t) e(t)                      // O(D)
    P(t) ← (1/λ) [P(t-1) - K(t) φᵀ(t) P(t-1)]     // O(D²)
END FOR

OUTPUT: θ(final), P(final)</pre>
</div></div><h3 id="PerformanceVerificationProposal-ComplexityAnalysis">Complexity Analysis</h3><p><strong>Memory Complexity:</strong> O(D²) - completely independent of data stream length!</p><p><strong>Time Complexity per Update:</strong> O(nD + D²)</p><ul><li><p>Bottleneck is usually the O(D²) matrix operations</p></li><li><p>For high-dimensional inputs (large n), the O(nD) feature computation can dominate</p></li></ul><p><strong>Comparison Table:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">Method               | Memory    | Time/Update  | Nonlinear | Exact
---------------------|-----------|--------------|-----------|-------
Linear RLS           | O(n²)     | O(n²)        | No        | Yes
Kernel RLS           | O(T²)     | O(T²)        | Yes       | Yes  
RLS + RFF            | O(D²)     | O(nD + D²)   | Yes       | ~Yes*</pre>
</div></div><p>*Approximation error → 0 as D → ∞</p><h1 id="PerformanceVerificationProposal-FeatureSet">Feature Set</h1><h2 id="PerformanceVerificationProposal-Requirement">Requirement</h2><h2 id="PerformanceVerificationProposal-Configuration">Configuration</h2><h2 id="PerformanceVerificationProposal-GlobalParameters">Global Parameters</h2><h2 id="PerformanceVerificationProposal-TransactionAttributes">Transaction Attributes</h2><h2 id="PerformanceVerificationProposal-CacheStates">Cache States</h2><h2 id="PerformanceVerificationProposal-SnoopResponses">Snoop Responses</h2><h2 id="PerformanceVerificationProposal-ChildTransactions">Child Transactions</h2><h2 id="PerformanceVerificationProposal-StateSnapshot">State Snapshot</h2><h2 id="PerformanceVerificationProposal-PerformanceMetrics">Performance Metrics</h2><h2 id="PerformanceVerificationProposal-ExampleFeatureSet">Example Feature Set</h2><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="syntaxhighlighter-pre" data-syntaxhighlighter-params="brush: java; gutter: false; theme: Confluence" data-theme="Confluence">{
  &quot;txn_id&quot;: &quot;T1234&quot;,
  &quot;sim_info&quot;: {
    &quot;cycle_start&quot;: 1_000_000,
    &quot;cycle_end&quot;: 1_000_220,
    &quot;run_id&quot;: &quot;regress_2025_09_01_1&quot;
  },

  &quot;config&quot;: {
    &quot;num_coherent_masters&quot;: 2,
    &quot;num_noncoherent_masters&quot;: 3,
    &quot;num_periph_slaves&quot;: 4,
    &quot;num_mem_slaves&quot;: 2,
    &quot;protocols_present&quot;: [&quot;CHI&quot;,&quot;AXI&quot;],
    &quot;ioaiu_cache_on&quot;: true,
    &quot;dmi_cache_on&quot;: true,
    &quot;legato_vc_count&quot;: 4,
    &quot;max_queue_depth&quot;: 32
  },

  &quot;global_metrics_window&quot;: {
    &quot;window_cycles&quot;: 1000,
    &quot;avg_e2e_latency&quot;: 120.5,
    &quot;p95_latency&quot;: 210,
    &quot;total_throughput_bytes_per_window&quot;: 128000,
    &quot;system_throughput_bytes_per_cycle&quot;: 128.0,
    &quot;active_txns_global&quot;: 3,
    &quot;avg_buffer_utilization&quot;: 0.38
  },

  &quot;primary&quot;: {
    &quot;txn_id&quot;: &quot;T1234&quot;,
    &quot;origin_master&quot;: &quot;CHI1&quot;,
    &quot;dest_slave&quot;: &quot;AXI:MEM0&quot;,
    &quot;txn_type&quot;: &quot;READONCE&quot;,
    &quot;address&quot;: &quot;0x80000040&quot;,
    &quot;cache_line_id&quot;: &quot;0x800000&quot;,        // address &gt;&gt; cache_line_shift
    &quot;size_bytes&quot;: 64,
    &quot;start_cycle&quot;: 1_000_000,
    &quot;end_cycle&quot;: 1_000_220,
    &quot;e2e_latency_cycles&quot;: 220,
    &quot;effective_bandwidth_B_per_cycle&quot;: 0.291,  // 64 bytes / 220 cycles
    &quot;path&quot;: [
      &quot;CHI1&quot;,
      &quot;CHI_to_SMI_conv&quot;,   // CHI → internal SMI
      &quot;LEGATO_seg0&quot;,
      &quot;DCE&quot;,
      &quot;LEGATO_seg1&quot;,
      &quot;DMI&quot;,
      &quot;AXI_MEM0&quot;
    ]
  },

  &quot;internal_commands&quot;: [
    {
      &quot;id&quot;: &quot;cmd_req_01&quot;,
      &quot;type&quot;: &quot;cmd_req&quot;,
      &quot;attributes&quot;: { &quot;opcode&quot;: &quot;READ&quot;, &quot;priority&quot;: 2, &quot;size&quot;: 64 },
      &quot;start_cycle&quot;: 1_000_002,
      &quot;end_cycle&quot;: 1_000_012,
      &quot;latency_cycles&quot;: 10
    },
    {
      &quot;id&quot;: &quot;cmd_rsp_01&quot;,
      &quot;type&quot;: &quot;cmd_rsp&quot;,
      &quot;attributes&quot;: { &quot;status&quot;: &quot;OK&quot;, &quot;grant&quot;: true },
      &quot;start_cycle&quot;: 1_000_013,
      &quot;end_cycle&quot;: 1_000_028,
      &quot;latency_cycles&quot;: 15
    }
  ],

  &quot;secondary_transactions&quot;: [
    {
      &quot;id&quot;: &quot;snp_01&quot;,
      &quot;initiator&quot;: &quot;DCE&quot;,
      &quot;target_master&quot;: &quot;CHI2&quot;,
      &quot;type&quot;: &quot;SNOOP&quot;,
      &quot;snoop_type&quot;: &quot;ReadOnce&quot;,
      &quot;start_cycle&quot;: 1_000_014,
      &quot;end_cycle&quot;: 1_000_050,
      &quot;latency_cycles&quot;: 36,
      &quot;snoop_response&quot;: {
        &quot;responder&quot;: &quot;CHI2_VIP&quot;,
        &quot;resp_type&quot;: &quot;UC_ACK&quot;,
        &quot;start_cycle&quot;: 1_000_047,
        &quot;end_cycle&quot;: 1_000_050,
        &quot;latency_cycles&quot;: 3
      }
    }
  ],

  &quot;snoop_summary&quot;: {
    &quot;snoop_fanout_count&quot;: 1,
    &quot;snoop_max_latency&quot;: 36,
    &quot;snoop_mean_latency&quot;: 36,
    &quot;snoop_response_counts&quot;: { &quot;UC_ACK&quot;: 1, &quot;Shared&quot;: 0, &quot;Invalid&quot;: 0 }
  },

  &quot;coherence&quot;: {
    &quot;coherence_state_before&quot;: &quot;UC&quot;,       // Unique Clean at CHI2 for this line
    &quot;coherence_conflict&quot;: true
  },

  &quot;address_conflict&quot;: {
    &quot;addr_conflict_count&quot;: 1,
    &quot;addr_conflict_sources&quot;: 1,
    &quot;addr_conflict_type&quot;: &quot;READ/READ&quot;
  },

  &quot;cache_outcome&quot;: {
    &quot;ioaiu_cache_hit&quot;: false,
    &quot;dmi_cache_hit&quot;: false,
    &quot;downstream_cmds_generated&quot;: 2
  },

  &quot;pressure&quot;: {
    &quot;LEGATO_seg0.vc0_qdepth_p95&quot;: 5,
    &quot;LEGATO_seg1.vc1_qdepth_p95&quot;: 3,
    &quot;SMI.link0_util&quot;: 0.72,
    &quot;DMI.cache_miss_rate&quot;: 0.35,
    &quot;IOAIU.MSHR_occupancy&quot;: 6
  },

  &quot;targets&quot;: {
    &quot;e2e_latency&quot;: 220,
    &quot;per_if&quot;: {
      &quot;CHI1.service_latency&quot;: 12,
      &quot;CHI_to_SMI_conv.service_latency&quot;: 5,
      &quot;LEGATO_seg0.service_latency&quot;: 20,
      &quot;DCE.service_latency&quot;: 80,
      &quot;LEGATO_seg1.service_latency&quot;: 15,
      &quot;DMI.service_latency&quot;: 60,
      &quot;AXI_MEM0.service_latency&quot;: 28
    }
  },

  &quot;notes&quot;: &quot;Txn completed; flagged as training sample candidate (labelled).&quot;
}
</pre>
</div></div><h1 id="PerformanceVerificationProposal-BlockDiagram">Block Diagram</h1><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250904-032024.png" width="828" src="https://arterisip.atlassian.net/wiki/download/attachments/1552515302/image-20250904-032024.png?api=v2"></span><p /><h1 id="PerformanceVerificationProposal-Flow">Flow</h1><ul><li><p>After the simulation begins, start sending transactions as usual</p></li><li><p>After the transaction finishes, the 'Performance Analyzer' (which is basically a scoreboard in UVM), captures the required data and passes it on to an external script and waits for the response</p></li><li><p>This script executes the model and gathers the prediction which is then sent back to the 'Performance Analyzer' through the DPI port</p></li><li><p>The analyzer will then compare the prediction with the observed values and then fail the test if they dont match or proceed further if they match (within an adjustable threshold)</p></li><li><p>NOTE: We will execute the model once after every transaction to avoid simulation slowdown</p></li></ul><h1 id="PerformanceVerificationProposal-Training">Training</h1><h2 id="PerformanceVerificationProposal-BulkTraining">Bulk Training</h2><ul><li><p>We first have to run a testcase which sends 1000s of transactions.</p></li><li><p>Each transaction would dump a feature set and hence we will have 1000s of feature set</p></li><li><p>We have to manually segregate the feature sets into ‘good' and 'bad’ numbers and then use these sets to train the model</p></li><li><p>This is an optional step to avoid frequent false failures in the beginning of the simulations</p></li></ul><h2 id="PerformanceVerificationProposal-ModelUpdate">Model Update</h2><ul><li><p>Assuming the model is trained enough, we should rarely see the test failing due to performance number mismatches</p></li><li><p>However, when we do see a mismatch and deem that it is a wrong prediction by the model, we need to feed the feature set to the model and update it so that the model understands this new corner case</p></li><li><p>If it is an RTL issue, we can fix the RTL and need not update the model</p></li></ul><h1 id="PerformanceVerificationProposal-ExternalScripts">External Scripts</h1><h1 id="PerformanceVerificationProposal-Challenges">Challenges</h1><h1 id="PerformanceVerificationProposal-NextSteps">Next Steps</h1>